\chapter{任务定义及评价方法}

根据“文本对子”之间的匹配关系，问答匹配任务被分为“问题-答案”匹配的答案选择任务以及“问题-问题”匹配的问句复述识别任务。
本文旨在研究这两个子任务上的现有难点，并提出一种基于数据增强和问句意图强化的问答匹配方法。
在接下来的内容中，我们将详细介绍这两个子任务的定义、语料资源以及常用评价指标。

\section{任务定义}

\subsection{答案选择任务定义}

答案选择任务主要目标是从大规模的候选答案中挖掘与给定问题相关的正解，常用于信息检索、筛选以及自动问答等场景。
为了完成答案选择任务，通常将其转化为目标问题与每个候选答案之间的二元分类任务，并通过分类得分对候选答案进行相关度排序。
在实际应用中，为了保证答案的准确性，通常会将相关度高的答案置前，而将相关度低的答案置后。
在实验中，我们的评价方法即是检验排序列表中的相关答案的排序位置，以此来验证答案选择模型的准确性和有效性。
表~\ref{table2-1}~给出了答案选择任务的数据样例，其中标签为1，代表该回答为给定问题相关的正解，而标签0代表其为非正解，即仅有第一个回答是与目标问题相关的答案。
该任务需要模型将三个候选答案分别与给定问题进行相关度匹配的计算，理想情况下，模型应给予正解最高的打分，排序至最前。

具体地，给定一个问句$Q$,以及n个候选答案$A={A_1, A_2,..., A_n}$，以及候选答案对应的标签$Y={y_1, y_2,..., y_n}$。
我们分别将其组成三元组形式的训练数据$(Q, A_1, y_1), (Q, A_2, y_2),..., (Q, A_n, y_n)$。
答案选择模型的目标是学习一个相关性打分函数$f$，该函数输入问句$Q$和候选答案$A_k$，输出其相关性打分$S_k$：
\begin{equation}
    f(Q, A_k) \rightarrow S_k
\end{equation}
根据对不同的答案输出的相关性打分，对候选答案进行排序后返回即可。
% 答案选择是问答匹配研究领域的子任务之一，其目标是面向特定问题从大规模候选答案中挖掘相关的正解。
% 答案选择可降解为目标问题与每个候选答案之间的二元分类任务，进而根据分类得分对候选答案进行相关度排序。
% 因此，实验将重点检验排序列表中的相关答案是否得以置前排列（即高相关度的答案置前，否则置后）。
% 如表~\ref{table2-1}~数据样例（1表示正解，0表示非正解），目标问题{\kai“SQL 2005 能做什么”}的三个候选答案中，仅有第一个答案正面回答了该问题。
% 答案选择模型需要对三个候选答案分别进行相关度计算，最终将正解排列在候选答案的最前面。

\input{table/table2-1.tex}

\subsection{问句复述识别任务定义}

问题复述识别任务需要判断两个问句的语义是否一致，常用于搜索引擎，智能客服等场景。
通常我们在网络上有许多已知答案的问题（例如百度知道，知乎等），该任务模型比对用户输入的问题与已知答案的问句进行匹配，将与用户问句语义尽量一致的候选问题的答案反馈给用户。
该子任务相较于答案选择任务，其对语义感知的精细程度的要求更为严格。
问句复述识别任务通常也同样使用二元分类任务的范式进行处理，将两个问句作为模型的输入，输出“复述”或“非复述”的二相判别结果。
例如表~\ref{table2-2}~中包含了中文以及英文的问句复述样本示例，其中1表示复述关系，0表示非复述关系。
第一个例子中，两个问句语义一致，都是在询问云南的有名特产。
而第二个例子中，一个问句在问“祛痘”的方法，另一个则在问“祛斑”，语义不同。
同样第三个英文例子中，一方在询问晚餐的“时间”，而另一方询问晚餐的“地点”。
% 问题复述识别模型的输入及运算模式与答案选择任务高度相似，可降解为目标问题与候选问题之间的二元分类任务。
% 该任务旨在判断两个自然问句的语义是否等价或具有较高相似度，即对输入的问句对子进行“复述”和“非复述”的二相判别。
% 表~\ref{table2-2}~罗列了一些问题复述识别样本（1表示复述关系，0表示非复述关系）。
% 其中，第一个例子{\kai“40$\times$30等于多少？”}与{\kai“30$\times$40等于多少？”}，这两个问句表达的意思完全相同，为复述关系。
% 而第三个例子中的{\kai“苹果”}与{\kai“香蕉”}存在明显的语义差异，为非复述关系。

\input{table/table2-2.tex}

\section{语料资源概述}
\label{2.2 语料资源概述}

对于答案选择任务，我们在公开的段落级语料WPQA（Wiki-PassageQA，简称WPQA）\cite{cohen2018wikipassageqa}以及短文本语料Trec-QA\cite{wang2007jeopardy}进行相关实验.
对于问句复述识别任务，我们在公开的中文问句复述语料LCQMC（Large-scale Chinese Question Matching Corpus，简称LCQMC）\cite{liu2018lcqmc}，BQ（Bank Question corpus）\cite{chen2018bq}以及英文问句复述语料QQP（Quora Question Pair dataset）\cite{shankar2017first}上对我们的方法进行实验验证。
本节将分别介绍这五个数据集的详细信息。


\subsection{答案选择数据集WPQA}

面向非事实性问答场景的答案选择数据集WPQA提供段落级候选答案，通常由多个句子构成。
WPQA中的问句由亚马逊的数据众包平台（Amazon’s Mechanical Turk）\footnote{https://www.mturk.com/}的工作者根据维基百科的文章创建，每个工作人员需要创建五个非事实性问题并标记出对应的答案段落。
该数据集包含4,165个非事实性问题，每个问题的候选答案中存在多个相关答案段落与多个非相关答案段落，且所有候选答案均来自于同一篇文章，因此候选答案之间存在高度的上下文相似性。
WPQA数据集中的答案皆由6句话组成，且99.9\%的答案段落长度在400词以内。
该数据集中的数据按照80\%、10\%、10\%的比例划分，分别用于训练、开发和测试，表~\ref{table2-3}~列出了数据集的统计信息。

\input{table/table2-3.tex}

\subsection{问题复述数据集LCQMC}


LCQMC包含约26万个问题对，每对问题拥有一个类别标签，表示该问题对是否互为复述关系。
LCQMC的构造者广泛收集不同领域（日常生活、教育、娱乐、计算机游戏、社会、自然科学和体育等）的高频词汇，每个领域选取50个高频词送入百度知道\footnote{https://zhidao.baidu.com/}查询出该领域下用户的真实提问，
然后采用一定策略选取出较为近似的问题组成候选问题对。
最终，每条候选问题对由三名众包工作者投票判断两者的是否为复述关系。
本文沿用前人的数据划分，将该数据集中的数据按照一定比例划分为训练集、开发集和测试集，表~\ref{table2-4}~列出了LCQMC数据集的统计信息。
其中，正样本表示复述问题对，负样本表示非复述问题对。

\input{table/table2-4.tex}

\section{性能评价指标}
\label{2.3 性能评价指标}

本文采用平均化倒数排序（Mean Reciprocal Rank，简称MRR）和平均化精度均值（Mean Average Precision，简称MAP）两个排序指标衡量答案选择模型对候选答案的排序能力，
并采用准确率（Accuracy，简称Acc）评估问题复述识别模型的二元分类能力。
同时，本文使用显著性检验（Statistical Significance Testing）\cite{dror2018hitchhiker}指标P-value衡量模型的性能提升是否显著。
本节将分别介绍上述评价指标。

\subsection{平均化倒数排序MRR}

MRR评价指标仅关注第一个相关答案的排名，取所在位置的倒数（Reciprocal Rank，简称RR）为值，对所有问题的$RR$求平均即为$MRR$。
其计算公式如下：
\begin{equation}
    RR = \frac{1}{rank}
\end{equation}
\begin{equation}
    MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|}RR_i
\end{equation}
其中，公式 2.1 中的$rank$表示与目标问题最相关的正解答案被模型排序后的位置，如果最相关的结果在第一条，则$rank$为1，即$RR$值为满分1。
公式 2.2 中的$|Q|$表示数据集中问题的数量，$RR_i$表示问题集合$Q$中的第$i$个问题的倒数排序$RR$值。

\subsection{平均化精度均值MAP}

MAP评价指标考虑目标问题的所有正解候选答案排序后的排名，计算得到其精度均值（Average Precision，简称AP），对所有问题的$AP$求平均即为$MAP$。
其计算公式如下：
\begin{equation}
    AP = \frac{sum_{k=1}^{|A^+|}k/rank_k}{|A^+|}
\end{equation}
\begin{equation}
    MAP = \frac{1}{|Q|} \sum_{i=1}^{|Q|}AP_i
\end{equation}
其中，公式 2.3 中的$|A^+|$表示候选答案队列中正解的数量，$k$表示第$k$个正解，$rank_k$表示第$k$个正解在模型排序结果中的位置。
公式 2.4 中的$|Q|$表示数据集中问题的数量，$AP_i$表示问题集合$Q$中的第$i$个问题的精度均值$AP$值。

\subsection{准确率Acc}

准确率是最常见的分类评价指标，其计算公式为模型分类正确的样本数量除以总样本数量。
通常来说，准确确率越高，分类模型能力越强。其计算公式如下：
\begin{equation}
    ACC = \frac{TP + TN}{P + N}
\end{equation}
其中，$P$表示正例样本数量，$N$表示负例样本数量，$TP$和$TN$分别表示模型预测正确的正负例样本数量。


\subsection{显著性检验指标P-value}

显著性分析能够验证所提方法性能提升是否显著，确保实验结果并非偶然。
本文采用Johnson\cite{johnson1999insignificance}建议的P-value阈值，将其设置为0.05，P-value小于阈值表示结果存在显著差异，否则差异不显著。
显著性检验需要将优化前后模型的多次实验性能进行对比计算得到P-value值，本文的显著性检验均借助开源工具\footnote{https://github.com/rtmdrr/testSignificanceNLP}完成。

\section{本章小节}

本章首先介绍了答案选择以及问题复述识别任务的定义，然后介绍了相关数据集的数据来源、构造方式及特点。
最后，介绍了问答匹配领域中的常用评价指标，包括Acc、MRR、MAP以及显著性检验指标P-value值。