\chapter{任务定义及评价方法}

根据“文本对子”之间的匹配关系，问答匹配任务被分为“问题-答案”匹配的答案选择任务以及“问题-问题”匹配的问句复述识别任务。
本文旨在研究这两个子任务上的现有难点，并提出一种基于数据增强和问句意图强化的问答匹配方法。
在接下来的内容中，我们将详细介绍这两个子任务的定义、语料资源以及常用评价指标。

\section{任务定义}

\subsection{答案选择任务定义}

答案选择任务主要目标是从大规模的候选答案中挖掘与给定问题相关的正解，常用于信息检索、筛选以及自动问答等场景。
为了完成答案选择任务，通常将其转化为目标问题与每个候选答案之间的二元分类任务，并通过分类得分对候选答案进行相关度排序。
在实际应用中，为了保证答案的准确性，通常会将相关度高的答案置前，而将相关度低的答案置后。
在实验中，我们的评价方法即是检验排序列表中的相关答案的排序位置，以此来验证答案选择模型的准确性和有效性。
表~\ref{table2-1}~给出了答案选择任务的数据样例，其中标签为1，代表该回答为给定问题相关的正解，而标签0代表其为非正解，即仅有第一个回答是与目标问题相关的答案。
该任务需要模型将三个候选答案分别与给定问题进行相关度匹配的计算，理想情况下，模型应给予正解最高的打分，排序至最前。

具体地，给定一个问句$Q$,以及n个候选答案$A={A_1, A_2,..., A_n}$，以及候选答案对应的标签$Y={y_1, y_2,..., y_n}$。
我们分别将其组成三元组形式的训练数据$(Q, A_1, y_1), (Q, A_2, y_2),..., (Q, A_n, y_n)$。
答案选择模型的目标是学习一个相关性打分函数$f$，该函数输入问句$Q$和候选答案$A_k$，输出其相关性打分$S_k$：
\begin{equation}
    f(Q, A_k) \rightarrow S_k
\end{equation}
根据对不同的答案输出的相关性打分，对候选答案进行排序后返回即可。
% 答案选择是问答匹配研究领域的子任务之一，其目标是面向特定问题从大规模候选答案中挖掘相关的正解。
% 答案选择可降解为目标问题与每个候选答案之间的二元分类任务，进而根据分类得分对候选答案进行相关度排序。
% 因此，实验将重点检验排序列表中的相关答案是否得以置前排列（即高相关度的答案置前，否则置后）。
% 如表~\ref{table2-1}~数据样例（1表示正解，0表示非正解），目标问题{\kai“SQL 2005 能做什么”}的三个候选答案中，仅有第一个答案正面回答了该问题。
% 答案选择模型需要对三个候选答案分别进行相关度计算，最终将正解排列在候选答案的最前面。

\input{table/table2-1.tex}

\subsection{问句复述识别任务定义}

问题复述识别任务需要判断两个问句的语义是否一致，常用于搜索引擎，智能客服等场景。
通常我们在网络上有许多已知答案的问题（例如百度知道，知乎等），该任务模型比对用户输入的问题与已知答案的问句进行匹配，将与用户问句语义尽量一致的候选问题的答案反馈给用户。
该子任务相较于答案选择任务，其对语义感知的精细程度的要求更为严格。
问句复述识别任务通常也同样使用二元分类任务的范式进行处理，将两个问句作为模型的输入，输出“复述”或“非复述”的二相判别结果。
例如表~\ref{table2-2}~中包含了中文以及英文的问句复述样本示例，其中1表示复述关系，0表示非复述关系。
第一个例子中，两个问句语义一致，都是在询问云南的有名特产。
而第二个例子中，一个问句在问“祛痘”的方法，另一个则在问“祛斑”，语义不同。
同样第三个英文例子中，一方在询问晚餐的“时间”，而另一方询问晚餐的“地点”。

具体地，给定两个问句$Q_1$和$Q_2$，以及其对应的是否为复述的二院标签$y$，组成三元组形式的训练数据$(Q_1, Q_2, y)$。
问句复述识别模型的目标是获得一个语义一致性的判别模型$f$，该函数以两个问句$Q_1$和$Q_2$作为输入，输出二元的一致性判别结果$S$：
\begin{equation}
    f(Q_1, Q_2) \rightarrow S
\end{equation}

% 问题复述识别模型的输入及运算模式与答案选择任务高度相似，可降解为目标问题与候选问题之间的二元分类任务。
% 该任务旨在判断两个自然问句的语义是否等价或具有较高相似度，即对输入的问句对子进行“复述”和“非复述”的二相判别。
% 表~\ref{table2-2}~罗列了一些问题复述识别样本（1表示复述关系，0表示非复述关系）。
% 其中，第一个例子{\kai“40$\times$30等于多少？”}与{\kai“30$\times$40等于多少？”}，这两个问句表达的意思完全相同，为复述关系。
% 而第三个例子中的{\kai“苹果”}与{\kai“香蕉”}存在明显的语义差异，为非复述关系。

\input{table/table2-2.tex}

\section{语料资源概述}
\label{2.2 语料资源概述}

对于答案选择任务，我们在公开的段落级语料WPQA（Wiki-PassageQA，简称WPQA）\cite{cohen2018wikipassageqa}以及短文本语料Trec-QA\cite{wang2007jeopardy}进行相关实验。
对于问句复述识别任务，我们在公开的中文问句复述语料LCQMC（Large-scale Chinese Question Matching Corpus，简称LCQMC）\cite{liu2018lcqmc}，BQ（Bank Question corpus）\cite{chen2018bq}以及英文问句复述语料QQP（Quora Question Pair dataset）\cite{shankar2017first}上对我们的方法进行实验验证。
本节将分别介绍这五个数据集的详细信息。


\subsection{答案选择数据集WPQA和Trec-QA}

% 面向非事实性问答场景的答案选择数据集WPQA提供段落级候选答案，通常由多个句子构成。
% WPQA中的问句由亚马逊的数据众包平台（Amazon’s Mechanical Turk）\footnote{https://www.mturk.com/}的工作者根据维基百科的文章创建，每个工作人员需要创建五个非事实性问题并标记出对应的答案段落。
% 该数据集包含4,165个非事实性问题，每个问题的候选答案中存在多个相关答案段落与多个非相关答案段落，且所有候选答案均来自于同一篇文章，因此候选答案之间存在高度的上下文相似性。
% WPQA数据集中的答案皆由6句话组成，且99.9\%的答案段落长度在400词以内。
% 该数据集中的数据按照80\%、10\%、10\%的比例划分，分别用于训练、开发和测试，表~\ref{table2-3}~列出了数据集的统计信息。

WPQA数据集是一个针对非事实性问答场景的答案选择数据集，提供了段落级的候选答案。
在该数据集中，问句是由亚马逊的数据众包平台（Amazon’s Mechanical Turk）根据维基百科的文章创建的，并且标记出对应的答案段落。
其总共包含4,165个非事实性问题，每个问题的候选答案中都包含多个相关答案段落和多个非相关答案段落。
值得注意的是，所有的候选答案都来自于同一篇文章，因此候选答案之间存在高度的相关性，对于深度学习模型具有着很大的挑战性。
公开版的数据集按照80\%、10\%、10\%的比例划分了训练集、开发集和测试集。
%，表格~\ref{table2-3}~展示了WPQA的统计信息。
% 该数据集的使用可以帮助我们更好地研究非事实性问答场景下的答案选择问题，并探究如何更好地处理段落级的候选答案。
% 此外，我们也可以利用该数据集来训练和评估问答匹配模型，以提高其在实际应用场景中的性能表现。

TrecQA数据集是由TREC问答竞赛产生的另一个面向问答场景的答案选择型数据集。
不同于WPQA，该数据集中候选答案的平均长度较短。
在本研究中，我们使用了公开的清理后的数据集，对数据集中没有答案，或只有正例或者负例答案的问题进行筛选过滤，去除了这部分样例。
表格~\ref{table2-3}~展示了WPQA和Trec-QA的样本分布和数据统计信息。


\input{table/table2-3.tex}

\subsection{问题复述数据集LCQMC，BQ和QQP}

LCQMC是一份大规模的开放域中文问句复述匹配语料，其数据包含约26万个问题对和类别标签，其中，每个二元标签代表该问句对子是否为复述。
这些问题是从中文的问答网站（如百度知道和知乎）收集而来，数据呈现出人们常用的多样化的自然语言形式。
其问句内容所涉及的领域覆盖广泛，例如生活常识、教育、体育、游戏和知识分享等，且每条问题对子由三名众包工作者投票其是否为复述关系。
其数据涉猎领域广，需要模型同时具备通用的语义常识和精细的语义感知能力。
本文沿用前人公开的划分方式，将数据按照固定的比例划分为训练集，开发集和测试集。

相比之下，BQ是一个特定领域的语料库，包含各种与银行业务相关的问题。
其问句的特点呈现出较为简洁的特点，大多为用户向客服发起的提问。
同样，本文也沿用前人公开的划分方式，将数据按一定比例划分为训练集，开发集和测试集。

Quora是目前公开的最大的英文场景下的问句复述识别数据集，其来源于Quora\footnote{https://www.quora.com}（国外类似知乎的网站）。
Quora为QQP数据集提供了大约40万个问题对，每个问题对都由两个问句组成。
这些问题对来自于Quora上用户提交的问题，以及从Quora网站上随机选择的问题，这样做的目的是为了尽可能地提高数据集的代表性。
在标注数据时，Quora聘请了许多标注者，这些标注者会对每个问题对进行标注，将其标记为相似或不相似。
表格~\ref{table2-4}~三个数据集的样本分布和统计信息，其中，正样本表示复述，负样本表示非复述。

\input{table/table2-4.tex}

\section{性能评价指标}
\label{2.3 性能评价指标}

本文采用平均化倒数排序（Mean Reciprocal Rank，简称MRR）和平均化精度均值（Mean Average Precision，简称MAP）两个排序指标衡量答案选择模型对候选答案的排序能力，
并采用准确率（Accuracy，简称Acc）评估问题复述识别模型的二元分类能力。
同时，本文使用显著性检验（Statistical Significance Testing）\cite{dror2018hitchhiker}指标P-value衡量模型的性能提升是否显著。
本节将分别介绍上述评价指标。

\subsection{平均化倒数排序MRR}

MRR评价指标仅关注第一个相关答案的排名，取所在位置的倒数（Reciprocal Rank，简称RR）为值，对所有问题的$RR$求平均即为$MRR$。
其计算公式如下：
\begin{equation}
    RR = \frac{1}{rank}
\end{equation}
\begin{equation}
    MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|}RR_i
\end{equation}
其中，公式 2.1 中的$rank$表示与目标问题最相关的正解答案被模型排序后的位置，如果最相关的结果在第一条，则$rank$为1，即$RR$值为满分1。
公式 2.2 中的$|Q|$表示数据集中问题的数量，$RR_i$表示问题集合$Q$中的第$i$个问题的倒数排序$RR$值。

\subsection{平均化精度均值MAP}

MAP评价指标考虑目标问题的所有正解候选答案排序后的排名，计算得到其精度均值（Average Precision，简称AP），对所有问题的$AP$求平均即为$MAP$。
其计算公式如下：
\begin{equation}
    AP = \frac{sum_{k=1}^{|A^+|}k/rank_k}{|A^+|}
\end{equation}
\begin{equation}
    MAP = \frac{1}{|Q|} \sum_{i=1}^{|Q|}AP_i
\end{equation}
其中，公式 2.3 中的$|A^+|$表示候选答案队列中正解的数量，$k$表示第$k$个正解，$rank_k$表示第$k$个正解在模型排序结果中的位置。
公式 2.4 中的$|Q|$表示数据集中问题的数量，$AP_i$表示问题集合$Q$中的第$i$个问题的精度均值$AP$值。

\subsection{准确率Acc}

准确率是最常见的分类评价指标，其计算公式为模型分类正确的样本数量除以总样本数量。
通常来说，准确确率越高，分类模型能力越强。其计算公式如下：
\begin{equation}
    ACC = \frac{TP + TN}{P + N}
\end{equation}
其中，$P$表示正例样本数量，$N$表示负例样本数量，$TP$和$TN$分别表示模型预测正确的正负例样本数量。


\subsection{显著性检验指标P-value}

显著性分析能够验证所提方法性能提升是否显著，确保实验结果并非偶然。
本文采用Johnson\cite{johnson1999insignificance}建议的P-value阈值，将其设置为0.05，P-value小于阈值表示结果存在显著差异，否则差异不显著。
显著性检验需要将优化前后模型的多次实验性能进行对比计算得到P-value值，本文的显著性检验均借助开源工具\footnote{https://github.com/rtmdrr/testSignificanceNLP}完成。

\section{本章小节}

本章首先介绍了答案选择以及问题复述识别任务的定义，然后介绍了相关数据集的数据来源、构造方式及特点。
最后，介绍了问答匹配领域中的常用评价指标，包括Acc、MRR、MAP以及显著性检验指标P-value值。